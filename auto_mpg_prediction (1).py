# -*- coding: utf-8 -*-
"""Auto MPG prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zE0DBjXCM90og788Lqi1B8UP2z6pjOdE
"""

# Importing Necessary Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

# Load the Auto MPG dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'car name']
auto_df = pd.read_csv(url, names=column_names, delim_whitespace=True, na_values='?')

# Drop rows with missing values
auto_df = auto_df.dropna()

# Drop the 'car name' column
auto_df = auto_df.drop('car name', axis=1)

# Encode 'origin' as it's a categorical variable
auto_df['origin'] = auto_df['origin'].astype('category')
auto_df = pd.get_dummies(auto_df, drop_first=True)

# Features (X) and Target (y)
X = auto_df.drop('mpg', axis=1)
y = auto_df['mpg']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Function to calculate regression metrics
def regression_metrics(y_true, y_pred):
    metrics = {
        'R-squared': r2_score(y_true, y_pred),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MSE': mean_squared_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))
    }
    return metrics

# Define and train various regression models
regression_models = {
    'Linear Regression': LinearRegression(),
    'KNN Regressor': KNeighborsRegressor(),
    'SVM Regressor': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'Random Forest Regressor': RandomForestRegressor(),
    'Gradient Boosting Regressor': GradientBoostingRegressor(),
    'Neural Network Regressor': MLPRegressor(max_iter=1000)
}

# Dictionary to store the results
regression_results = {}

# Loop through models, train, and evaluate them
for name, model in regression_models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Store the performance metrics
    regression_results[name] = regression_metrics(y_test, y_pred)

# Convert the results to a DataFrame for better visualization
regression_results_df = pd.DataFrame(regression_results).T
print("\nModel Performance Metrics:")
print(regression_results_df)

# Number of iterations for testing model stability
num_iterations = 30  # Adjust as needed

# Initialize dictionaries to store cumulative metrics for each model
cumulative_results = {model: {'R-squared': [], 'MAE': [], 'MSE': [], 'RMSE': []} for model in regression_models.keys()}

# Run multiple iterations
for i in range(num_iterations):
    # Split the data (shuffle with each iteration)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Standardize the features
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Loop through models, train, and evaluate them
    for name, model in regression_models.items():
        # Train the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Collect performance metrics
        metrics = regression_metrics(y_test, y_pred)
        for metric_name, value in metrics.items():
            cumulative_results[name][metric_name].append(value)

# Calculate average and standard deviation for each metric
average_results = {}
for model, metrics in cumulative_results.items():
    average_metrics = {f'avg_{metric}': np.mean(values) for metric, values in metrics.items()}
    std_metrics = {f'std_{metric}': np.std(values) for metric, values in metrics.items()}
    average_results[model] = {**average_metrics, **std_metrics}

# Convert the results to a DataFrame for better visualization
average_results_df = pd.DataFrame(average_results).T
print("\nAverage Model Performance Metrics across iterations:")
print(average_results_df)

import matplotlib.pyplot as plt
# Set up the figure and axes for the plots
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

# Plot R-squared values
axs[0, 0].barh(regression_results_df.index, regression_results_df['R-squared'], color='skyblue')
axs[0, 0].set_title('R-squared Values of Regression Models')
axs[0, 0].set_xlabel('R-squared')

# Plot Mean Absolute Error
axs[0, 1].barh(regression_results_df.index, regression_results_df['MAE'], color='salmon')
axs[0, 1].set_title('Mean Absolute Error of Regression Models')
axs[0, 1].set_xlabel('MAE')

# Plot Mean Squared Error
axs[1, 0].barh(regression_results_df.index, regression_results_df['MSE'], color='lightgreen')
axs[1, 0].set_title('Mean Squared Error of Regression Models')
axs[1, 0].set_xlabel('MSE')

# Plot Root Mean Squared Error
axs[1, 1].barh(regression_results_df.index, regression_results_df['RMSE'], color='gold')
axs[1, 1].set_title('Root Mean Squared Error of Regression Models')
axs[1, 1].set_xlabel('RMSE')

# Adjust layout
plt.tight_layout()
plt.show()

import seaborn as sns

#Bar Plot for R-squared values
plt.figure(figsize=(10, 6))
plt.barh(regression_results_df.index, regression_results_df['R-squared'], color='teal')
plt.title('R-squared Values of Regression Models')
plt.xlabel('R-squared')
plt.tight_layout()
plt.show()

#Box Plot for MAE, MSE, and RMSE
plt.figure(figsize=(10, 6))
sns.boxplot(data=regression_results_df[['MAE', 'MSE', 'RMSE']], palette="Set2")
plt.title('Box Plot of MAE, MSE, and RMSE')
plt.ylabel('Error Metrics')
plt.tight_layout()
plt.show()

#Radar Chart for all metrics
labels = regression_results_df.index  # Model names
metrics = ['R-squared', 'MAE', 'MSE', 'RMSE']  # Metrics to plot
num_vars = len(metrics)

# Create a radar chart
angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]  # Close the loop for angles

# Set up the radar plot
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

# Iterate over each model's metrics
for i in range(len(labels)):
    # Get the stats for each model
    stats = regression_results_df.iloc[i][metrics].values.flatten().tolist()
    stats += stats[:1]

    ax.fill(angles, stats, alpha=0.25)
    ax.plot(angles, stats, label=labels[i])
ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics, fontsize=12)
ax.set_title('Radar Chart for Regression Models', size=15)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

#Line Plot for MAE, MSE, RMSE
plt.figure(figsize=(10, 6))
plt.plot(regression_results_df.index, regression_results_df['MAE'], marker='o', label='MAE', color='orange')
plt.plot(regression_results_df.index, regression_results_df['MSE'], marker='o', label='MSE', color='blue')
plt.plot(regression_results_df.index, regression_results_df['RMSE'], marker='o', label='RMSE', color='green')
plt.title('Line Plot of Error Metrics by Model')
plt.xlabel('Regression Models')
plt.ylabel('Error Metrics')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Model Performance with Fewer Features:

The Random Forest Regressor outperforms the other models in the Auto MPG dataset with the highest R-squared value of 0.886865, indicating that it explains a significant proportion of variance in the target variable (MPG). It also has the lowest MAE, MSE, and RMSE, showing it provides the best overall predictions.
The KNN Regressor also performs well, closely following the Random Forest with high accuracy and low error metrics.
"""

# -*- coding: utf-8 -*-
"""Auto MPG prediction with Multiple Iterations.ipynb"""

# Importing Necessary Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Auto MPG dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'car name']
auto_df = pd.read_csv(url, names=column_names, delim_whitespace=True, na_values='?')

# Drop rows with missing values
auto_df = auto_df.dropna()

# Drop the 'car name' column
auto_df = auto_df.drop('car name', axis=1)

# Encode 'origin' as it's a categorical variable
auto_df['origin'] = auto_df['origin'].astype('category')
auto_df = pd.get_dummies(auto_df, drop_first=True)

# Features (X) and Target (y)
X = auto_df.drop('mpg', axis=1)
y = auto_df['mpg']

# Standardize the features
scaler = StandardScaler()

# Function to calculate regression metrics
def regression_metrics(y_true, y_pred):
    metrics = {
        'R-squared': r2_score(y_true, y_pred),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MSE': mean_squared_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))
    }
    return metrics

# Define and initialize regression models
regression_models = {
    'Linear Regression': LinearRegression(),
    'KNN Regressor': KNeighborsRegressor(),
    'SVM Regressor': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'Random Forest Regressor': RandomForestRegressor(),
    'Gradient Boosting Regressor': GradientBoostingRegressor(),
    'Neural Network Regressor': MLPRegressor(max_iter=1000)
}

# Number of iterations for testing model stability
num_iterations = 30

# Initialize dictionaries to store cumulative metrics for each model
cumulative_results = {model: {'R-squared': [], 'MAE': [], 'MSE': [], 'RMSE': []} for model in regression_models.keys()}

# Run multiple iterations
for i in range(num_iterations):
    # Split the data (shuffle with each iteration)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Standardize the features
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Loop through models, train, and evaluate them
    for name, model in regression_models.items():
        # Train the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Collect performance metrics
        metrics = regression_metrics(y_test, y_pred)
        for metric_name, value in metrics.items():
            cumulative_results[name][metric_name].append(value)

# Calculate average and standard deviation for each metric
average_results = {}
for model, metrics in cumulative_results.items():
    average_metrics = {f'avg_{metric}': np.mean(values) for metric, values in metrics.items()}
    std_metrics = {f'std_{metric}': np.std(values) for metric, values in metrics.items()}
    average_results[model] = {**average_metrics, **std_metrics}

# Convert the results to a DataFrame for better visualization
average_results_df = pd.DataFrame(average_results).T
print("\nAverage Model Performance Metrics across iterations:")
print(average_results_df)

# Visualization: Bar Plot for R-squared values
plt.figure(figsize=(10, 6))
plt.barh(average_results_df.index, average_results_df['avg_R-squared'], color='teal')
plt.title('Average R-squared Values of Regression Models')
plt.xlabel('R-squared')
plt.tight_layout()
plt.show()

# Box Plot for MAE, MSE, and RMSE
plt.figure(figsize=(10, 6))
sns.boxplot(data=average_results_df[['avg_MAE', 'avg_MSE', 'avg_RMSE']], palette="Set2")
plt.title('Box Plot of Average MAE, MSE, and RMSE')
plt.ylabel('Error Metrics')
plt.tight_layout()
plt.show()

# Radar Chart for average metrics across models
labels = average_results_df.index  # Model names
metrics = ['avg_R-squared', 'avg_MAE', 'avg_MSE', 'avg_RMSE']  # Metrics to plot
num_vars = len(metrics)

# Create a radar chart
angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]  # Close the loop for angles

# Set up the radar plot
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

# Iterate over each model's metrics
for i in range(len(labels)):
    stats = average_results_df.iloc[i][metrics].values.flatten().tolist()
    stats += stats[:1]
    ax.fill(angles, stats, alpha=0.25)
    ax.plot(angles, stats, label=labels[i])

ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(['R-squared', 'MAE', 'MSE', 'RMSE'], fontsize=12)
ax.set_title('Radar Chart for Average Metrics of Regression Models', size=15)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

# Line Plot for Average MAE, MSE, RMSE
plt.figure(figsize=(10, 6))
plt.plot(average_results_df.index, average_results_df['avg_MAE'], marker='o', label='MAE', color='orange')
plt.plot(average_results_df.index, average_results_df['avg_MSE'], marker='o', label='MSE', color='blue')
plt.plot(average_results_df.index, average_results_df['avg_RMSE'], marker='o', label='RMSE', color='green')
plt.title('Line Plot of Average Error Metrics by Model')
plt.xlabel('Regression Models')
plt.ylabel('Average Error Metrics')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()